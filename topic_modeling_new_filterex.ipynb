{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cravi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom stopwords\n",
    "custom_stopwords = {\"o\", \"d\", \"D\", \"v\", \"est\", \"ad\", \"qui\", \"non\", \"ut\",\n",
    "                     \"sed\", \"deß\", \"et\", \"te\", \"at\", \"quae\", \"cum\", \"sic\",\n",
    "                       \"hoc\", \"m\", \"--\", \"Herr\", \"herr\", \"Gott\", \"gott\"} \n",
    "for word in custom_stopwords:\n",
    "    nlp.Defaults.stop_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text_spacy(text):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords, remove Latin sentences, and lemmatize German text using spaCy.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zäöüß\\s.!?\\\\/]', '', text)  # Preserve German characters an slashes for sentence separation\n",
    "    text = re.sub(r'\\b\\w\\b', '', text)  # Remove single-character words\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text files from folder\n",
    "def load_documents(folder_path):\n",
    "    \"\"\"\n",
    "    Load all .txt files from the specified folder and return a list of documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and create dictionary & corpus\n",
    "def preprocess_and_create_corpus(documents):\n",
    "    \"\"\"\n",
    "    Preprocess documents and create dictionary and corpus for topic modeling.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text...\")\n",
    "    processed_docs = [preprocess_text_spacy(doc) for doc in documents]\n",
    "\n",
    "    print(\"Creating dictionary and corpus...\")\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    dictionary.filter_extremes(no_below=3, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    return processed_docs, dictionary, corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run preprocessing\n",
    "def run_preprocessing(folder_path, num_topics=5, num_words=10):\n",
    "    # Load documents\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents(folder_path)\n",
    "    print(f\"{len(documents)} documents loaded.\")\n",
    "\n",
    "    # Preprocess and create dictionary & corpus\n",
    "    processed_docs, dictionary, corpus = preprocess_and_create_corpus(documents)\n",
    "\n",
    "    return dictionary, corpus, processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train LDA model and display topics\n",
    "def train_lda(corpus, dictionary, num_topics=5, num_words=10, passes=10, random_state=100):\n",
    "    \"\"\"\n",
    "    Train an LDA model using the given corpus and dictionary.\n",
    "    \"\"\"\n",
    "    print(f\"Training LDA model with {num_topics} topics...\")\n",
    "    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes, random_state=random_state, workers=12)\n",
    "    print(\"Num_topics \", num_topics, \", passes\", passes, \", random_state \", random_state)\n",
    "    print(\"Topics:\")\n",
    "    for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "    return lda_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "445 documents loaded.\n",
      "Preprocessing text...\n",
      "Creating dictionary and corpus...\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder path\n",
    "folder_path = \"corpus_norm\"\n",
    "\n",
    "# Run topic modeling\n",
    "dictionary, corpus, processed_docs = run_preprocessing(folder_path, num_topics=15, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lda(corpus, dictionary, passes_list, topics_list, output_folder=\"topics_no_herrgott_filt\"):\n",
    "    \"\"\"\n",
    "    Train LDA models with different passes and topic numbers, then save results.\n",
    "\n",
    "    Args:\n",
    "        corpus (list): The bag-of-words representation of the corpus.\n",
    "        dictionary (gensim.corpora.Dictionary): The dictionary mapping word IDs to words.\n",
    "        passes_list (list): List of values for passes.\n",
    "        topics_list (list): List of values for number of topics.\n",
    "        output_folder (str): Folder to save the output files.\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all combinations of passes and topics\n",
    "    for passes in passes_list:\n",
    "        for num_topics in topics_list:\n",
    "            print(f\"Training LDA model with {num_topics} topics and {passes} passes...\")\n",
    "\n",
    "            # Train LDA model using multiple cores\n",
    "            lda_model = LdaMulticore(\n",
    "                corpus=corpus, id2word=dictionary,\n",
    "                num_topics=num_topics, passes=passes, random_state=42, workers=6\n",
    "            )\n",
    "\n",
    "            # Define file paths\n",
    "            base_filename = f\"lda_topics_{num_topics}topics_{passes}passes\"\n",
    "            txt_filepath = os.path.join(output_folder, f\"{base_filename}.txt\")\n",
    "            model_filepath = os.path.join(output_folder, f\"{base_filename}.model\")\n",
    "\n",
    "            # Save topics to a text file\n",
    "            with open(txt_filepath, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(f\"LDA Model with {num_topics} Topics and {passes} Passes\\n\")\n",
    "                file.write(\"=\" * 50 + \"\\n\")\n",
    "                for idx, topic in lda_model.print_topics(num_words=10):\n",
    "                    file.write(f\"Topic {idx}: {topic}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "            # Save the trained LDA model\n",
    "            lda_model.save(model_filepath)\n",
    "\n",
    "            print(f\"Saved: {txt_filepath}\")\n",
    "            print(f\"Saved model: {model_filepath}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model with 5 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_5topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_5topics_10passes.model\n",
      "Training LDA model with 10 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_10topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_10topics_10passes.model\n",
      "Training LDA model with 20 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_20topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_20topics_10passes.model\n",
      "Training LDA model with 30 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_30topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_30topics_10passes.model\n",
      "Training LDA model with 40 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_40topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_40topics_10passes.model\n",
      "Training LDA model with 50 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_50topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_50topics_10passes.model\n",
      "Training LDA model with 100 topics and 10 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_100topics_10passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_100topics_10passes.model\n",
      "Training LDA model with 5 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_5topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_5topics_15passes.model\n",
      "Training LDA model with 10 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_10topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_10topics_15passes.model\n",
      "Training LDA model with 20 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_20topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_20topics_15passes.model\n",
      "Training LDA model with 30 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_30topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_30topics_15passes.model\n",
      "Training LDA model with 40 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_40topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_40topics_15passes.model\n",
      "Training LDA model with 50 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_50topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_50topics_15passes.model\n",
      "Training LDA model with 100 topics and 15 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_100topics_15passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_100topics_15passes.model\n",
      "Training LDA model with 5 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_5topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_5topics_20passes.model\n",
      "Training LDA model with 10 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_10topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_10topics_20passes.model\n",
      "Training LDA model with 20 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_20topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_20topics_20passes.model\n",
      "Training LDA model with 30 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_30topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_30topics_20passes.model\n",
      "Training LDA model with 40 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_40topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_40topics_20passes.model\n",
      "Training LDA model with 50 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_50topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_50topics_20passes.model\n",
      "Training LDA model with 100 topics and 20 passes...\n",
      "Saved: topics_no_herrgott_filt\\lda_topics_100topics_20passes.txt\n",
      "Saved model: topics_no_herrgott_filt\\lda_topics_100topics_20passes.model\n"
     ]
    }
   ],
   "source": [
    "# Define parameter values\n",
    "passes_list = [10, 15, 20]\n",
    "topics_list = [5, 10, 20 , 30, 40, 50, 100]\n",
    "\n",
    "train_and_save_lda(corpus, dictionary, passes_list, topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
